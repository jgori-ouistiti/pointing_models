% !TEX root = main.tex

% \documentclass[12pt,a4paper]{article}
\documentclass[manuscript,review,anonymous]{acmart}

% Rights management information.  This information is sent to you
% when you complete the rights form.  These commands have SAMPLE
% values in them; it is your responsibility as an author to replace
% the commands and values with those provided to you when you

% complete the rights form.\
\copyrightyear{2024}
\acmYear{2024}
\acmConference[CHI '24]{Proceedings of the CHI Conference on Human Factors in Computing Systems}{May 11--16, 2024}{Honolulu, HI, USA}
\acmBooktitle{Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI '24), May 11--16, 2024, Honolulu, HI, USA}
\acmDOI{XXXXXXXXXXXX}
\acmISBN{XXXXXXXXXXXX}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{lmodern}
\usepackage[all]{foreign}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{colortbl}
% \usepackage[left=3cm,right=2cm,top=2cm,bottom=2cm]{geometry}

%%%%%%%%%%%%%%%%%%%%%
\usepackage[commandnameprefix = always]{changes}
\usepackage{here}
\usepackage{comment}
\usepackage{xspace}
\usepackage{textgreek}
\usepackage{enumitem}


\title{Pointing Models for Users Operating Under Different Speed Accuracy Strategies}
\author{Julien Gori}
\email{julien.gori@sorbonne-universite.fr}
\orcid{0000-0002-3272-9176}
\affiliation{%
  \institution{Sorbonne Universit√©, CNRS, ISIR}
  \city{Paris}
  \country{France}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Custom commands
\newcommand{\mmt}{\ensuremath{\overline{\text{MT}}}\xspace}
\newcommand{\mt}{\ensuremath{{\text{MT}}}\xspace}
\newcommand{\ide}{\ensuremath{{\text{ID}_e}}\xspace}
\newcommand{\strat}{\,\text{strat}\xspace}



\DeclareMathOperator*{\SumInt}{%
\mathchoice%
  {\ooalign{$\displaystyle\sum$\cr\hidewidth$\displaystyle\int$\hidewidth\cr}}
  {\ooalign{\raisebox{.14\height}{\scalebox{.7}{$\textstyle\sum$}}\cr\hidewidth$\textstyle\int$\hidewidth\cr}}
  {\ooalign{\raisebox{.2\height}{\scalebox{.6}{$\scriptstyle\sum$}}\cr$\scriptstyle\int$\cr}}
  {\ooalign{\raisebox{.2\height}{\scalebox{.6}{$\scriptstyle\sum$}}\cr$\scriptstyle\int$\cr}}
}

\begin{document}


\begin{abstract}
	This work proposes and compares models for pointing tasks, focusing on users operating under different speed-accuracy strategies. Traditional models like Fitts' law capture the relationship between movement time and target difficulty but often overlook strategic aspects and only describe average performance. We address this by 1) capturing the statistical dependence between speed and accuracy 2) developing parametric models that describe the entire distribution of movement times for different user strategies. We namely utilize copulas, a novel approach in HCI, alongside other more established techniques, to model pointing data comprehensively. We use existing datasets to measure dependencies and compare our models. This work advances the knowledge of pointing models and provides the required resources to implement the suggested models.
\end{abstract}
\maketitle


\section{Introduction\label{sec:introduction}}
Pointing towards a target \eg selecting an icon with a desktop mouse, is one of the fundamental interactions in HCI~\cite{soukoreff2004, card1978}. Pointing \textit{models} are regularly used to evaluate input techniques~\cite{soukoreff2004} and to design interfaces~\cite{cockburn2007}.
Classically, HCI researchers model pointing time with Fitts' law~\cite{fitts1954, soukoreff2004, gori2018tochi}, and refer to it as \textit{movement time} MT
\begin{align}
	\text{MT} & = a + b \log_2 (1 + \text{D}/\text{W}), \label{eq:fitts} \\
	          & \equiv a + b\,\text{ID}.
\end{align}
Here, D and W are geometric parameters (size and distance of the target), $\text{ID}=\log_2 (1 + \text{D}/\text{W})$ is the index of difficulty, and $a$ and $b$ are parameters estimated from empirical data.
Fitts' law captures the speed-accuracy tradeoff of human movement: decreasing W will lead to larger MT \ie a higher accuracy can be obtained only at the cost of longer pointing times.


Since Fitts' seminal 1954 work~\cite{fitts1954}, and its introduction by Card and colleagues in HCI in 1978~\cite{card1978}, movement modeling has greatly advanced.
While Fitts' law only describes \emph{endpoints}, motor control models now propose description of entire \emph{trajectories} produced during pointing tasks~\cite{todorov2002, saveriano2023, fischer2021, gori2020, gori2023}.
While Fitts' law only describes \emph{means} of movement times, several models now describe \emph{distribution} of movement times, usually using asymmetric distributions~\cite{chapuis2007, jude2016, gori2019,li2024,zhao2022, nieuwenhuizen2016}.



One aspect that has mostly escaped attention in Fitts' law modeling is the strategic aspect of pointing. Most people will regularly over or under utilize the target width W~\cite{soukoreff2004}, creating, according to Zhai and colleagues, \textit{an additional subjective layer of speed-accuracy tradeoff}~\cite{zhai2004nominal}, which the current formulations of Fitts' law do not capture \textit{a priori}. But models that describe how users point under different speed-accuracy strategies are essential when addressing users with varying incentives.
For example, people will favor accuracy in contexts where safety matters critically~\cite{guiard2011b}.



One notable exception is the Weighted Homographic (WHo) model of Guiard and Rioul~\cite{guiard2015}, which quantitatively relates movement time, movement precision, strategy, and effort (a detailed explanation of the model is provided \autoref{app:who}). The WHo model was recently used as a low level control module in hierarchical models of user behavior~\cite{jokinen2021, langerak2022}, where a high level controller learns to be more cautious in contexts where the cost to recover from errors is higher.
However, we believe the WHo model is not an appropriate tool to describe user performance, since it was built as a model of \textit{ultimate} performance.
It does not describe the entire distribution of data, but instead, it describes the limit speed and accuracy performances one can obtain(the \textit{front of performance}, in Guiard and Rioul's words). In Guiard and Rioul's work, the WHo model is fitted only on the best 10 measures, out of 400! Hence, works that use the WHo model use an overly optimistic model of user behavior.


The goal of this work is to propose and compare candidate models for pointing for users operating under different speed-accuracy strategies. Our constraints are as follows:
\begin{itemize}
	\item The model should describe the entire distribution of movement times, and not just the means (as in Fitts' law) or extrema (as in WHo, also see~\cite{gori2017two, gori2019, li2024}). This ensures the model will be widely applicable.
	\item The model should be parametric, so that it can be easily interpreted, re-used and adapted by others \eg as a low-level control module as in~\cite{jokinen2021,langerak2022} without too much effort.
	\item One of the parameters of the model should map directly to the concept of strategy \ie a bias towards speed or accuracy when producing movements, filling the gap identified beforehand.
\end{itemize}


A central aspect of this work that should be highlighted is that we focus on characterizing the relationship between speed and accuracy, with particular attention to their (statistical) dependence. When participants prioritize accuracy, there is an average increase in pointing time, which leads to a broader pointing time distribution~\cite{gori2019, li2024, zhao2022}\footnote{As hypothesized by \cite[p. 132]{gori2018these} in analogy to reaction time distributions~\cite{wagenmakers2007}.}. Understanding precisely how this spread increases is central to our analysis. Therefore, we make use of copulas, a statistical tool well-suited for capturing the dependence between variables which to our knowledge has never been used in HCI before, to model the pointing data, among other better-known techniques.





\paragraph{Notations} $\mathcal{N}(\mu, \Sigma)$ refers to the (multivariate) Gaussian distribution with mean $\mu$ and covariance matrix $\Sigma$, $\mathcal{E}(\lambda)$ refers to the exponential distribution with mean $\lambda$, $\mathbb{E}[X]$ refers to the mathematical expectation of $X$, $\text{Var}(X)$ refers to the variance of $X$.

\section{Background}


\subsection{Participant strategies and the effective index of difficulty}
Fitts' law experiments are usually conducted to investigate/evaluate the speed accuracy tradeoff. To do so, the experimenter manipulates accuracy by modifying W and observes speed of the response by measuring MT in \autoref{eq:fitts}. However, it is well known that this manipulation is imperfect: users underutilize \textit{easy} (low ID) targets and over-utilize \textit{hard} (high ID) targets~\cite{guiard2011,zhai2004nominal}.
A solution, due to Crossman and popularized in HCI by Mackenzie~\cite{gori2018tochi} is to compute an effective index of difficulty \ide  as a \textit{post-hoc} adjustment of difficulty as a function of the effectively measured standard deviation of endpoints $\sigma$

\begin{align}
	\text{MT} & = a + b\,\text{ID}_e                                                   \\
	          & = a + b\,\log_2 \left(1 + \frac{D}{4.133\sigma}\right). \label{eq:ide}
\end{align}

Several researchers, beginning with Mackenzie~\cite{mackenzie2008} have investigated how people perform Fitts' task when instructed to emphasize either speed or accuracy. As expected, participants are able to modify their strategies, as reflected by changes in \ide for a given nominal ID level.
For example in Mackenzie's experiment, on average participants erred up to 20\% in the speed condition, but erred close to 0\% in the accuracy condition. Guiard~\cite{guiard2011} later performed a similar experiment, but without explicit width (participants had to aim towards a line) with 5 instructions (Ultra Fast, Fast, Balanced, Accurate, Ultra Accurate). Other similar experiments include [Ufuk, Wolfgang, Shota].
Fitts' law, even with \ide, does not link strategy, \ide and MT \textit{a priori}; it can only measure \ide \textit{after} the experiment has been conducted.


\subsection{Measures of dependence between two random variables\label{subs:rw::dependence}
}

\subsubsection{Measures of association}
Measures of association~\cite{nelsen2006} quantify the strength and direction of the relationship between two variables; they are (in most cases) weaker than measures of dependence in the sense that independence implies an absence of association, but the converse is not true.

\paragraph{Pearson's $r$} Pearson's $r$ is a well-known measure of association that captures the \textit{linear relationship} between two random variables. A value of $1$ (resp. $-1$) means that the data from the two variables are perfectly aligned with a positive (resp. negative) slope. One well-known weakness of Pearson correlation is that a null value only indicates the absence of a linear association, even though non-linear associations are common.

\paragraph{Spearman's $\rho$ and Kendall's $\tau$}
Spearman's $\rho$ between two variables equals Pearson's $r$ between the \textit{ranks} of the variables; as a result it allows capturing monotonic (increasing or decreasing, which includes linear but many more) relationships between variables. It will have a value of $1$ (resp. $-1$) when the order of the sorted values of one variable is the same (resp. opposite) as the other variable.
Kendall's $\tau$ is a different rank correlation metric, which, for our purpose, has the same properties as $\rho$\footnote{\dots{} but in general do not have the same value~\cite[Chapter 5]{nelsen2006}. It is more sensitive to smaller rank inversions or ties. Both $\rho$ and $\tau$ can be derived from the same general correlation coefficient~\cite{kendall1948}}.



% These measures of association (usually $\tau$) can sometimes be mapped to the copula's parameters $\theta$, for example for the Gumbel copula specified above $\tau = \frac{\theta -1}{\theta}$, meaning that the dependence between two variables can be steered by a few parameters only (in this case 1).


\subsubsection{Bivariate copulas \label{subsub:copula}}
Bivariate copulas are a statistical tool that describe the dependence between two random variables~\cite{nelsen2006}.
They ``couple''~\cite[Chapter 1]{nelsen2006} two marginals into a single joint distribution, ``parametrically'', where the parameter of the copula controls the strength of the association between the variables.
Different types of copula introduce different types of dependence; for example a Gumbel copula introduces so-called upper tail dependence~\cite{nelsen2006}, where if one variable takes on a high value, the other is also likely to take on a high value.
Identifying the best fitting copula fully describes the dependence structure between two variables, which include linear/non linear and tail dependences, and encapsulates rank correlations such as $\tau$ and $\rho$ \eg a Gumbel copula of parameter $\theta$ has $\tau  = \frac{\theta -1}{\theta}$. However, to fully characterize data, it has to be coupled with the marginals of each variable, which specify the means, variances, and more generally the distributions of these.

% As an example, consider the marginal distributions of human height and weight. While taller individuals tend to be heavier on average, sampling independently of the height and weight marginals would disregard this relationship, leading to unrealistic combinations, where taller and smaller people would have the same chance of being associated with a particular weight. Copulas address this issue by introducing a dependence structure between the variables, allowing for a more realistic joint distribution that reflects the correlation between height and weight; that dependence can be steered by the copula's parameters. Later in this work, we will use the one parameter Gumbel copula, which has cumulative density function $C_{\theta}(u,v) = \exp \left( -\left[ (-\log u) ^{\theta} + (-\log v)^{\theta} \right]^{1/\theta} \right)$.




\subsection{Fitts' law and regression \label{subs:rw::fitts}}
The expression for Fitts' law \autoref{eq:ide} masks an underlying statistical model that assumes additive, centered noise\footnote{Because most researchers applying Fitts' law rely on ordinary least squares (OLS), they implicitly assume the noise is uncorrelated and homoscedastic, as OLS is only optimal under these conditions.}:
\begin{align} \text{MT} = a + b\ide + \text{noise (additive)}, \quad \mathbb{E}[\text{noise}] = 0 \text{ (centered)}. \end{align}
In linear models, Pearson's $r$ is commonly used as a goodness-of-fit measure, particularly because in this case, $r^2 = R^2$, reflecting the proportion of variance in the dependent variable that is explained by the model and confirming the model‚Äôs linearity~\cite{lee1988}. However, as shown by Drewes~\cite{drewes2013} and Gori~\etal~\cite{gori2018chi}, the Pearson correlation between MT and \ide, $r(\text{MT}, \ide)$, often yields low values in practice, with some cases showing correlations close to zero~\cite{gori2018tochi}.

To mitigate variability in MT and increase the goodness-of-fit measure, researchers typically compute the sample average of MT, denoted in the remainder of the work as \mmt\footnote{See~\cite{gori2017two} for further discussion on considering the sample average.}, before calculating $r$. Consequently, the model actually used is:
\begin{align}
	\mmt = a + b\ide + \text{noise (additive)}, \quad \mathbb{E}[\text{noise}] = 0 \text{ (centered)}, \label{eq:mmt}
\end{align}
with the goodness-of-fit measure being $r(\mmt, \ide)$.


\subsection{Asymmetric MT distributions and the Exponentially Modified Gaussian noise model \label{subs:rw::emg}}
Rather than averaging out the noise as in \autoref{eq:mmt}, recent works model the noise distribution instead, and show that the conditional distribution of MTs for a given ID or \ide level is distributed according to an asymmetric, positive (right) skewed~\cite{gori2018these,gori2019,zhao2022, jude2016,nieuwenhuizen2016, li2024,chapuis2007} distribution.
While several distributions may be possible, including gamma, Gumbel and lognormal, we follow Gori and colleagues~\cite{gori2018these,gori2019, li2024}, and Zhao \etal~\cite{zhao2022} who considered an Exponentially Modified Gaussian (EMG) noise model as MT distribution: it satisfies all the mentioned properties, has parameters that are easy to interpret, has shown good performance (including when modeling steering data~\cite{wang2021}), and there exist tools to aid in parameter estimation~\cite{li2024,emgregs}.


Gori~\cite{gori2018these} further showed that a model where the standard deviation of the MT distribution increases linearly with its mean provided a superior fit to empirical data:
\begin{align}
	 & \text{MT}   = G + E, \label{eq:emg}                                                                 \\
	 & G           \sim \mathcal{N}(\beta \mathbf{x}, s) = a + b\,\ide + \mathcal{N}(0,s), \label{eq:emg1} \\
	 & E           \sim \mathcal{E}(\lambda \mathbf{x}), \label{eq:emg2}                                   \\
	 & \mathbf{x}  = [1, \ide]^t,~\beta = [a,b],~\lambda = [\lambda_0, \lambda_1].
\end{align}
Although the EMG model aforementioned has a quite complex density, it has very simple conditional mean
\begin{align}
	\mathbb{E}[\text{MT}|\ide] = \beta \mathbf{x} + \lambda \mathbf{x}, \label{eq:emg_mean}
\end{align}
and conditional variance
\begin{align}
	\text{Var}(\text{MT}|\ide) = s^2 + (\lambda \mathbf{x})^2.
\end{align}
Zhao \etal~\cite{zhao2022} confirmed the quality of the EMG fit using a similar model.

One sees the EMG distribution results from adding an exponential noise term \autoref{eq:emg2} to a more classical centered Gaussian noise model \autoref{eq:emg1}.
Depending on the parameter values, the EMG distribution can behave like a Gaussian ($s >> \lambda \mathbf{x}$), like an exponential ($s << \lambda \mathbf{x}$) or in between~\cite{gori2019}.


\subsection{Two Fitts' laws}
Gori and colleagues~\cite{gori2017,gori2018tochi} have argued that there exist two version of Fitts' law: an average Fitts' law ---which we call the ``mean''  law, computed via linear regression as in \autoref{subs:rw::fitts}, and a minimum Fitts' law, which we call the ``min'' law. The ``min'' law represents the best performance possible, whereas the mean law represents a more central tendency. The EMG model presented just before can be used to identify the min law~\cite{gori2019} via the $\beta$ parameters.






\section{Dependence between \ide and MT in a classical Fitts experiment \label{sec:jgp}}

\subsection{Theoretical analysis of $r(\mmt, \ide)$ correlation \label{subs:theory_rsq}}
We first investigate the type of dependence between $\ide$ and MT that is required to have data that is compatible with Fitts' law \ie $r^2(\ide, \mmt)$ close to 1. One way to indicate a dependence between \ide and MT is by specifying the conditional distribution of MT given \ide $p(MT | \ide)$.
In \hypertarget{res:method_2__correlation}{the appendix}, we show that \emph{any} conditional MT distribution with a mean linear in \ide will reach $r^2(\ide, \mmt) = 1$.

Below, we give some examples that fulfill this condition
\begin{itemize}
	\item The EMG distribution~\cite{gori2018these,gori2019, li2024,zhao2022} \autoref{eq:emg} has mean value $\beta \mathbf{x} +  \lambda \mathbf{x} = \beta_0 + \lambda_0 + (\beta_1 + \lambda_1) \ide $.
	\item A gamma~\cite{jude2016} distribution with shape $k$ and scale $\theta$ has mean value $k\theta$, so having $k$ or $\theta$ increase linearly with \ide will satisfice the condition.
	\item A generalized extreme value~\cite{chapuis2007} distribution GEV$(\mu, \sigma, s)$ has a mean value that increases linearly with \ide if $\mu = a + b \ide$.
\end{itemize}
Hence, a high $r^2(\ide, \mmt)$ is easily reachable by imposing a linear conditional mean, and many distributions remain possible.
This shows the $r(\mmt, \ide)$ condition is not enough to characterize the dependence between MT and \ide, because it does not consider the entire distribution, but only its first moment\footnote{$r(\text{MT}, \ide)$ additionally considers the second moment, but still not the entire distribution.}. We thus turn to the other classical measures of dependence introduced \autoref{subs:rw::dependence}.

% \subsection{Empirical analysis of $r(\mmt, \ide)$ correlation \label{subs:empirical_jgp}}
% To study the $r(\mmt, \ide)$ correlation, we used an existing dataset by Jude Guinness and Poor~\cite{jude2016} (JGP). The JGP dataset was generated using the well-known 2D multi-directional tapping task~\cite{soukoreff2004} via the FittsStudy software~\cite{wobbrock2011}, with the experiment replicated six times over three days, twice a day, for 15 participants. We noticed a bug in the original dataset, where the 2 last replications are identical to the two first, so we discarded the 2 last replications from the dataset, leaving us with 4 replications. In total, we thus have $60 = 15 \times 4$ individual datasets.


% \begin{itemize}
% 	\item compute likelihoods with constant model, linear model, quadratic model, square root model
% 	\item check other candidates [--]
% 	\item compute LR test for all
% \end{itemize}



% \bigskip



\subsection{Measures of association}

For each participant and each iteration, we computed Pearson's $r$, Spearman's $\rho$ and Kendall's $\tau$. Histograms of the three measures are provided \autoref{fig:association}, where we notice values typically associated with moderately/strong (rank) correlation. We also computed intraclass correlation coefficients (ICC), which all turned out significant ($\alpha=0.011$) (results available in the supplementary materials), indicating individual differences in association.
Hence, the JGP dataset pictures moderately strong correlations between MT and \ide for all participants, and differences between participants.
\begin{figure}[htbp]
	\centering
	\includegraphics[width=.5\textwidth]{img/association.pdf}
	\caption{Histogram of the measures of association (Pearson's $r$, Spearman's $\rho$ and Kendall's $\tau$).}
	\label{fig:association}
	\Description{Histogram of the measures of association (Pearson's $r$, Spearman's $\rho$ and Kendall's $\tau$).}
\end{figure}


\subsection{Fitting copulas \label{sub:jgp::fit_copula}}
The previous measures of association are helpful as a diagnostic, but they are difficult to exploit for creating a generative distribution for movement times. One possibility is to play with variance models for the conditional distribution of MT knowing \ide, and determine the best model based on maximum likelihood estimation.%, similar to our approach in \autoref{subs:empirical_jgp}. Here, we take a more principled approach, and try to match the structure of the dependence between \ide and MT with known copulas.

The methods used to fit copulas for all 60 individual datasets of the JGP dataset is described \autoref{app:copula_method}. The log-likelihoods, averaged over the 4 replications, and normalized by the maximum average log-likelihood for each participant are displayed \autoref{fig:copulas_jgp}. It shows the rotated Gumbel copula to systematically outperform other copulas. The rotated Galambos copula follows very closely; this is not surprising considering Gumbel and Galambos copula are very similar~\cite{genest2017}. The t-copula and rotated H√ºsler-Reiss (HR) copulas also score decently close to the rotated Gumbel. The Gumbel and t copulas are represented \autoref{fig:copulas}.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=\columnwidth]{img/ll_jgp.pdf}
	\caption{Log-likelihoods for each copula and for each participant. Closest to 1 (greenest) indicated best fit. The log-likelihoods are the result of averaging the log-likelihoods of the 4 replications. They are then normalized for each participant by dividing the log-likelihood by the maximum log-likelihood over all potential copulas.}
	\label{fig:copulas_jgp}
	\Description{Log-likelihoods for each copula and for each participant. Closest to 1 (greenest) indicated best fit. The log-likelihoods are the result of averaging the log-likelihoods of the 4 replications. They are then normalized for each participant by dividing the log-likelihood by the maximum log-likelihood over all potential copulas.}
\end{figure}










\section{Modeling the effect of user strategy on endpoints \label{sec:gop}}
The previous section investigated the nature of the dependence between \ide and MT for a given strategy \ie the one participants naturally choose during a pointing experiment. We now investigate the effect of strategy on that dependence.


\subsection{The GO dataset}
We investigate the effect of participant strategy on movement time and \ide, using the Guiard-Olafsdottir (GO) dataset~\cite{guiard2011}.
The pointing task used to create the GO dataset differs from a typical Fitts task as it lacks a predefined W (no nominal accuracy). Participants had to aim at a 1-pixel line, with five different strategies, from speed emphasis (allowing large under/overshoots) to precision emphasis (targeting a 1-pixel-wide line). The balanced condition represents a natural strategy, akin to a standard Fitts experiment, with speed and precision conditions falling between extremes.

An \ide can be calculated retrospectively, similar to Fitts' tasks. Previous research~\cite{guiard2011,gori2020} has shown that this data aligns with classical Fitts' law, with mean movement time (\mmt) strongly correlating with \ide.



\subsection{Correlation between \ide and \mmt}

A summary of the GO dataset, shown in \autoref{fig:go_ide}, plots \mmt against \ide for all participants, grouped by strategy. Participants covered a wide range of \ide (1 to 9 bits), depending on the strategy. The separation between strategies shows consistent self-regulation across participants, although with considerable overlap. \mmt globally increases with \ide across strategies, but also within, as indicated by the orientation\footnote{The covariance matrix of a Gaussian bivariate is the rightmost term of \autoref{eq:gaussian_ide_mt} where $\rho$ is the correlation between the two components. The angle of the prediction ellipsis is given by $\frac{1}{2} \arctan¬†\left( \frac{2\rho \sigma_i\sigma_t}{\sigma_i^2 - \sigma_t^2} \right) $.} of the 95\% prediction ellipses from the bivariate Gaussian fit to each strategy, indicating a ``local'' speed-accuracy tradeoff. The ``Speed emphasis condition'' is an exception, with \mmt and \ide being seemingly uncorrelated \ie when going really fast, accuracy becomes independent of local changes in speed.



\subsection{Association per strategy}
We also computed association measures for the GO dataset, per strategy, or aggregated for all strategies. The figures are available in the supplementary materials, and are not reproduced here: $r$, $\rho$ and $\tau$ have similar values to the ones of the JGP dataset (see \autoref{fig:association}), and the measures of associations are close to 0 for the speed emphasis strategy, positive but low (about 0.1 to 0.2) for the other strategies.

\subsection{Per-strategy bivariate Gaussian model for \mmt and \ide}
We model the joint increase of \mmt and \ide with a bivariate Gaussian model, for each of the five strategies of the GO dataset. We chose the bivariate model because it is simple, easy to interpret, it makes the dependence between \mmt and \ide completely specified via Pearson's $r$, and is expected to fit reasonably well given the visual appearance of the clusters. The fit for each strategy is displayed by drawing the associated $95\%$ prediction ellipsis, see \autoref{fig:go_ide}.

\begin{align}
	\begin{pmatrix}
		\ide \\
		\mmt
	\end{pmatrix} \sim \mathcal{N} \left( \begin{bmatrix}
			                                      \mu_i \\
			                                      \mu_t
		                                      \end{bmatrix}, \begin{bmatrix}
			                                                     \sigma^2_i          & r \sigma_i \sigma_t \\
			                                                     r \sigma_i \sigma_t & \sigma_t^2
		                                                     \end{bmatrix} \right). \label{eq:gaussian_ide_mt}
\end{align}
When the strategy emphasizes precision, the mean of the Gaussian vector increases, as well as the size of the prediction ellipsis; however, except for the Speed emphasis condition, its orientation stays relatively constant, suggesting that the correlation between the \ide and \mmt may also be.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=.8\columnwidth]{img/fitts_ide_go_with_ellipse.pdf}
	\caption{\mmt against \ide for all participants and strategies for the GO dataset where the color indicates strategy. A bivariate Gaussian fit per strategy is also displayed through the associated $95\%$ prediction ellipsis.}
	\label{fig:go_ide}
	\Description{\mmt against \ide for all participants and strategies for the GO dataset where the color indicates strategy. A bivariate Gaussian fit per strategy is also displayed through the associated $95\%$ prediction ellipsis.}
\end{figure}


% \begin{figure}[htbp]
% 	\centering
% 	\includegraphics[width=\columnwidth]{img/mean_cov.pdf}
% 	\caption{MT against \ide for the GO dataset for each strategy (from left, max emphasis on speed, to right, max emphasis on accuracy). The 95\% confidence ellipsis corresponding to the Gaussian fit is represented in red}
% 	\label{fig:meancov}
% \end{figure}

\subsection{Bivariate Gaussian model for \mmt and \ide parametrized by strategy}
To represent strategy numerically rather than ordinally, we map the five instructions to equally spaced points on the interval [-1,1]\footnote{The choice here may seem na√Øve, and we could have mapped strategy to the set of real values via \ide. However, our choice has two advantages: the mapping can be defined before movements are performed, and does not depend on each participant.}, with 0 as the balanced strategy, 1 for speed emphasis, and -1 for precision emphasis. We then assessed how the five parameters of the bivariate Gaussian model \autoref{eq:gaussian_ide_mt} aligned with this numerical strategy, by fitting a linear model on each parameter. The fits are displayed \autoref{fig:meancov_strat} and their summaries can be found in the supplementary materials.


\noindent We observe that:
\begin{itemize}
	\item The linear increase of the mean ($\mu_i$, $\mu_t$) with numerical strategy is visually clear, and supported by high goodness of fit.
	\item The increase of $\sigma_i$ and $\sigma_t$ with numerical strategy can also reasonably be described by a linear model, although evidence is less strong than for the mean.
	\item The correlation between \mmt and \ide can also be modeled by a linearly increasing model. However, the slope is not statistically significant (see Supplementary materials) and it is visually clear that speed and accuracy are uncorrelated. As another model, one may also consider a null correlation for the speed emphasis condition, and a constant correlation for the other conditions; we computed the mean $r$ for the 4 other conditions and obtained $r=0.44$. We choose the latter model, since the speed emphasis condition is unlikely to be of interest in HCI applications (in contrast with the accuracy emphasis condition).
\end{itemize}


\begin{figure}[htbp]
	\centering
	\makebox[\textwidth]{
		\includegraphics[width=1.3\columnwidth]{img/mean_cov_strat.pdf}
	}
	\caption{The parameters of the mean and covariance of the bivariate Gaussian model defined in \autoref{eq:gaussian_ide_mt} plotted against the five numerical strategies. The associated linear fits and $r^2$ are displayed above each plot.}
	\label{fig:meancov_strat}
	\Description{The parameters of the mean and covariance of the bivariate Gaussian model defined in \autoref{eq:gaussian_ide_mt} plotted against the five numerical strategies. The associated linear fits and $r^2$ are displayed above each plot.}

\end{figure}

\subsection{Fitting copulas}
Similarly to \autoref{sub:jgp::fit_copula}, we fit copulas on the GO dataset, per strategy and per participant. The results contrast somewhat with what we found in \autoref{sec:jgp}: this time the t-copula comes ahead; the rotated Gumbel is not a close second. This is true when looking on the basis of participants, aggregating over strategies (left panel of \autoref{fig:ll_cop_gop}) or per strategy, aggregating over participants (right panel of \autoref{fig:ll_cop_gop}).

\begin{figure}[htbp]
	\centering
	\makebox[\textwidth]{
		\includegraphics[height=.5\columnwidth]{img/ll_gop_per_P.pdf}
		\includegraphics[height=.5\columnwidth]{img/ll_gop_per_strat.pdf}
	}
	\caption{Log-likelihoods of the tested copulas, normalized for each row by the best fitting copula. Left panel: Comparison on the basis of participants, when data is aggregated over all strategies. Right panel: Comparison on the basis of strategies, when data is aggregated over all participants.}
	\label{fig:ll_cop_gop}
	\Description{Log-likelihoods of the tested copulas, normalized for each row by the best fitting copula. Left panel: Comparison on the basis of participants, when data is aggregated over all strategies. Right panel: Comparison on the basis of strategies, when data is aggregated over all participants.}
\end{figure}

In \autoref{sec:jgp}, we identified the rotated Gumbel as the best-fitting copula, while here, the t-copula is found to be the most suitable. Contour plots of the t-copula and rotated Gumbel copula, with various marginals, are shown in \autoref{fig:copulas} for parameters fit on a random subset from the JGP dataset.
The left panel illustrates a rotated Gumbel copula with standard normal marginals, which can be compared to the middle right panel displaying the t-copula with the same marginals. The t-copula is symmetric and implies equal (positive) tail dependencies, whereas the rotated Gumbel only captures lower tail dependencies\footnote{Lower (upper) tail dependencies refer to the correlation between only low (high) values of MT and \ide.}, making it initially surprising that these distinct copulas could both emerge as best candidates~\cite{demarta2005,nelsen2006}.

However, examining the middle left and right panels, which show the rotated Gumbel and t-copulas for marginals estimated from the JGP dataset, clarifies this. These contour plots model the joint distribution $p(MT,\ide)$ and appear quite similar, with increasing conditional variance for higher \ide values. A noticeable difference is that the t-copula's joint distribution assigns higher probabilities to MT for large \ide values compared to the rotated Gumbel.

Given the similarity in the resulting joint distributions and the t-copula's effectiveness in modeling the JGP dataset (see \autoref{fig:copulas_jgp}), we will use the t-copula to express the dependency between MT and \ide for the remainder of this work.

We also evaluated if there were any participant differences or strategy difference in terms of the parameters of the estimated t-copula, using the GO dataset. In fact, we found no differences for both parameters; more details are available in the supplementary materials. On average, the two parameters of the t-copula are $\rho_1 = 0.75$ and $\nu >> 1$, indicating that for many cases, the t-copula actually reduces to a Gaussian copula.\footnote{The parameter $\nu$ stands for the degrees of freedom of the t-copula; on average we found $\nu = 4056$. Just like a t-distribution converges to a Gaussian for many degrees of freedom, so does the t-copula converge to the Gaussian copula for high values of $\nu$.~\cite{demarta2005, nelsen2006}. It should be noted that the estimation of the average $\nu$ is somewhat unreliable for the current datasets, namely because the arithmetic average is not necessarily meaningful in this case, and because the number of data points is probably too limited to catch the fine structure of dependencies. When estimated on the entire GO dataset, we find $\nu = 16$.}




\begin{figure}[htbp]
	\centering
	\makebox[\textwidth]{
		\includegraphics[width=.3\columnwidth]{img/rotgumbel-normal-margins.pdf}
		\includegraphics[width=.3\columnwidth]{img/rotgumbel-estimated-margins.pdf}
		\includegraphics[width=.3\columnwidth]{img/t-normal-margins.pdf}
		\includegraphics[width=.3\columnwidth]{img/t-estimated-margins.pdf}
	}
	\caption{Density functions for four joint distributions constructed using various copulas and marginals. Left: the rotated Gumbel copula, with standard normal margins. Middle Left: the same copula with marginals estimated from the GO dataset. This represents a statistical model of the pointing data. Middle Right: the t-copula with standard normal margins. Right: the t-copula with marginals estimated from the GO dataset. This represents a competing statistical model for pointing data.}
	\label{fig:copulas}
	\Description{Density functions for four joint distributions constructed using various copulas and marginals. Left: the rotated Gumbel copula, with standard normal margins. Middle Left: the same copula with marginals estimated from the GO dataset. This represents a statistical model of the pointing data. Middle Right: the t-copula with standard normal margins. Right: the t-copula with marginals estimated from the GO dataset. This represents a competing statistical model for pointing data.}
\end{figure}



\section{Generating pointing data for different speed-accuracy strategies: Rationale and methods for the different models \label{sec:gen}}


We are now ready to return to our initial goal: generate pointing data that depend on user strategies, according to a parametric model. We suggest three methods; everywhere, $\Theta$ refers to a vector of parameters which includes, but is not limited to, strategy $\mathcal{S}$, $p_{\Theta}(\ide, \text{MT})$ to the joint distribution between \ide and MT parametrized by $\Theta$, $p_{\Theta}(\text{MT}|\ide)$ to the conditional distribution of MT given \ide.


\subsection{Model 1: Joint distributions using Copulas.}
\paragraph{The model.} The first method directly defines $p_{\Theta}(\ide, \text{MT})$ in terms of copulas: As explained in \autoref{subsub:copula}, the copula $C_{\Theta}$ links the marginals $F_{\Theta}(\ide)$ and $G_{\Theta}(\text{MT})$ into the joint $p_{\Theta}(\ide, \text{MT})$
\begin{align}
	p_{\Theta}(\ide, \text{MT}) = C_{\Theta}(F_{\Theta}(\ide), G_{\Theta}(\text{MT})).
\end{align}
Pointing data can then be obtained by sampling directly from the joint. This method assumes access to the marginals, which can be estimated from empirical data, or given/assumed.

\paragraph{Accounting for different strategies.} In the previous section, we did not find differences in the t-copula for different strategies. Hence, we only need to exploit the fact that the marginal \ide distribution, obtained by marginalizing \autoref{eq:gaussian_ide_mt}, is a factor of strategy $\mathcal{S}$ \ie
\begin{align}
	\ide \sim \mathcal{N}(4.72 + 2.3\,\mathcal{S}, 1.06 + 0.39\,\mathcal{S}). \label{eq:gauss_strategy_marginal}
\end{align}
We can then sample \ide values by drawing from the Gaussian \autoref{eq:gauss_strategy_marginal}, and use the conditional\footnote{The closed form formula of the conditional copula can be computed from any copula, see~\cite{nelsen2006}} t-copula to sample the associated MT values.

\paragraph{Inputs of the model.} To operationalize this model, one needs
\begin{enumerate}
	\item The parameters of the t-copula. We found $\rho_1 = .69$, $\nu = 16.9$ by estimating the t-copula on the entire GO dataset. Without access to routines to estimate copula parameters, $\rho_1$ can be deduced from Kendall's $\tau$ using $\rho_1 = \sin(\tau \frac{\pi}{2})$, and $\nu$ can be deduced from the upper and lower tail dependence coefficients using a more complex formula~\cite[Section 3]{demarta2005}.
	\item The parameters of the marginal distributions for MT and \ide. These can be simply estimated using any popular statistical package. For MT, several asymmetric distributions may be valid; we prefer the EMG distribution as explained \autoref{subs:rw::emg}. For the GO dataset, we found $\beta = 0.53$, $\sigma = 0.19$, and $\lambda= .75$.
	\item The levels and number of repetitions may also be given to generate repeated measures.
\end{enumerate}


\paragraph{Pro's and con's.} As explained \autoref{subsub:copula}, using copulas allows specifying directly the level of association between variables independently of the marginals. Hence, one can couple arbitrary marginals for \ide (\eg to fit a particular task) and MT (\eg using another asymmetric distribution for MT) with the same t-copula.
This first model based on copulas provides a principled way of identifying dependencies between \ide and MT, and enough flexibility in exploiting that dependency for generating data, including repeated measurements.

However, two negative points have to be highlighted. First, copulas have a good track record in modeling tail dependencies, but for pointing data the lower tail of the MT distribution should associate strongly with the entire \ide distribution~\cite{gori2018tochi} ---not only its tail---. Second, the tools to work with copulas remain somewhat underdeveloped, which may add barriers for some researchers. As an example, in writing this paper, we have used an R library for copulas, whereas the rest of the work was performed using Python.

\subsection{Model 2: Conditional distribution using EMG.}
\paragraph{The model.} The second method directly leverages the EMG model of \autoref{subs:rw::emg}. For each point, a value of $\ide$ is selected by sampling from the marginal $F_{\theta}(\ide)$; then a value of MT is selected by sampling from the conditional distribution
\begin{align}
	p_{\Theta}(\text{MT}|\ide) = \text{EMG}(\text{MT}; \ide, \Theta).
\end{align}

\paragraph{Accounting for different strategies.} Similar to the previous model, we can directly draw \ide values from drawing from the Gaussian \autoref{eq:gauss_strategy_marginal}, and sample MT from the conditional EMG distribution.

\paragraph{Inputs of the model.} To operationalize this model, one needs the parameters of the EMG distribution described \autoref{eq:emg} -- \autoref{eq:emg_mean}. We estimated these values for the GO dataset to be $\beta_0 = 0.08$, $\beta_1 = .14$, $\sigma=0.18$, $\lambda_0 = .17$, $\lambda_1 = .08$. The levels and number of repetitions may also be given to generate repeated measures.

\paragraph{Pro's and con's} This method will produce data that resembles empirical data by construction, since it relies directly on the EMG distribution, which has been shown to fit data well. One of the main weaknesses of this method is that it does not allow controlling the association between \ide and MT \hypertarget{res:method_2__correlation}{independently} of the EMG parameters, as shown in the proof \autoref{app:proofs}: the Pearson correlation $r$ is entirely determined by the EMG parameters. Additionally, as explained \autoref{subs:theory_rsq} it will likely produce values of $r(\mmt, \ide)$ much closer to 1 than what we found \autoref{fig:go_ide} and \autoref{fig:meancov_strat}.

\subsection{Model 3: Joint distribution of (\ide, \mmt).}


\paragraph{The model.} This method exploits the Gaussian bivariate model \autoref{eq:gaussian_ide_mt}. A couple ($\ide$ $\mmt$) is first sampled from the joint.
Then, because the EMG distribution parameters are related to \mmt by
\begin{align}
	\mmt & = \beta \mathbf{x} + \lambda \mathbf{x} = \beta_0 + \beta_1 \ide + \lambda_0 + \lambda_1 \ide,
\end{align}
we ``correct'' $\lambda_1$, ensuring it remains positive to keep the general trend of increasing conditional mean and variance, as indicated \eg by the fitted copulas \autoref{fig:copulas}.
\begin{align}
	\lambda_1 = \text{max}(0, \frac{\mmt - \beta_0 - \beta_1\,\ide}{\ide}). \label{eq:lambda_1}
\end{align}
We don't correct $\beta$ since this would alter Fitts' min law parameters, and don't alter $\lambda_0$ to ensure a minimum variance.
Movement times are then sampled from the EMG with corrected parameters. Notice in \autoref{eq:lambda_1} that we force $\lambda_1$ positive; this is to keep an increasing variance. This comes at the cost of occasional departures from the correction.

\paragraph{Accounting for different strategies.} Here, we draw samples directly from the bivariate Gaussian \autoref{eq:gaussian_ide_mt} with
\begin{align}
	\mu_i    & = 4.72 + 2.3\,\mathcal{S}        \\
	\mu_t    & = 1.3 + .64\,\mathcal{S}         \\
	\sigma_i & = 1.06 + 0.29\,\mathcal{S}       \\
	\sigma_t & = .39 + .08\,\mathcal{S}         \\
	r        & = \begin{cases}
		             0 \text{ if } \mathcal{S} = -1 \\
		             .44 \text{ else}
	             \end{cases}
\end{align}

\paragraph{Inputs of the model.} Same as for Model 2.

\paragraph{Pro's and con's} Similarly to the previous method, this method will produce data according to the EMG distribution, making it realistic. Contrary to method 2, we do control the association between \mmt and \ide in this third method. However, the needed correction is sometimes not fully applied (when $\lambda_1 = 0$ in \autoref{eq:lambda_1}, part of the \mmt has not been absorbed into $\lambda_1$); this method also does not really control how the movement time distribution is modified beyond the ability to fit \mmt.







\section{Model comparison}

\subsection{Consistency \label{subs:consistency}}
We first start by investigating the consistency of the different methods \ie how close the data produced by the generative models are to the reference dataset. We consider a dataset with large sample size so that our measures don't depend too much on random sampling\footnote{The data were generated with a fixed seed (1234). Replications with two other seeds (777, 999) and no seeds are available in the supplementary materials. The conclusions for these replications remain the same as for the results reported in the paper, indicating the sample size of the dataset was indeed large enough}.
The generated datasets are compared based on the following metrics:

\begin{enumerate}
	\item association measures: $r(\text{MT}, \ide)$, $\rho(\text{MT}, \ide)$, $\tau(\text{MT}, \ide)$,
	\item Fitts' law measures: parameters of the ``mean'' and the ``min'' law,
	\item estimation of the joint probability $p(\ide, \mmt)$ through $r(\overline{\text{MT}}, \ide)$,
	\item computation of ISO-throughput.
\end{enumerate}
The difference between the ``mean'' and ``min'' law has already been explained \autoref{subs:rw::emg}. We use ISO-throughput~\cite{soukoreff2004}, also known as the ``mean-of-means'' throughput, since it is used in practice by some HCI researchers as a one dimensional score to evaluate overall pointing performance. Hence, it gives a concise way of measuring differences between the GO dataset and the generated ones, which are expected to be interpretable and meaningful to HCI researchers.
It should be noted that, for maximum consistency, we consider the same \ide values as those from the original dataset for the generated data.\footnote{In other words there is no \ide variability, only MT variability in these generated datasets. All 3 models can accommodate this: for model 1 by sampling from the conditional copula, for model 3 by sampling from the conditional Gaussian bivariate, and for model 2 trivially.}



The results are summarized \autoref{fig:consistency}.
Overall, the differences between the models are minimal, as all models produce data that closely match the metrics of the original dataset. When considering measures of association, all models align well with the original dataset.
All models lead to slightly underestimated ISO throughput, with a maximum difference of 0.25 bit/s for model 1.
In terms of Fitts' law, this model maintains the mean law (OLS) but distorts the minimum law (EMG), resulting in a higher intercept and a smaller slope. Model 2, on the other hand, preserves the minimum law but slightly skews the mean law, yielding a higher intercept and lower slope. Model 3 comes closest to replicating the original dataset, with both laws nearly aligned; only the intercept in the EMG fit is slightly underestimated.
All models lead to inflated values of $\overline{r} = r(\mmt, \ide)$. For Model 2, this is expected due to its construction based on a linear conditional expectation. For Model 1, the high values of $\overline{r}$ are more surprising, but likely due to the symmetric nature of the t-copula.
Model 3 is the only one that reduces $\overline{r}$, although it still does not reach the target $\overline{r}$ due to the enforced positive $\lambda_1$. The supplementary materials demonstrate a $\beta_0$ correction that achieves the same $\overline{r}$ as the original dataset, but this comes at the expense of significantly poorer performance in other metrics.

A final observation concerns the lower tail of the joint distribution. For low \ide values, all models tend to generate MT values that are too low. Models 2 and 3 are particularly problematic in this regard, as they even predict some negative MT values.


\begin{figure}[htbp]
	\centering
	\makebox[\textwidth]{
		\includegraphics[width=1.3\columnwidth]{img/method_consistency.pdf}
	}
	\caption{Consistency as evaluated by the criterion of \autoref{subs:consistency}. The original dataset (left panel) is compared to data produced by model 1 (middle left panel), model 2 (middle right panel) and model 3 (right panel).}
	\label{fig:consistency}
	\Description{Consistency as evaluated by the criterion of \autoref{subs:consistency}. The original dataset (left panel) is compared to data produced by model 1 (middle left panel), model 2 (middle right panel) and model 3 (right panel).}
\end{figure}


\subsection{Effect of strategy}
We have evaluated the consistency of the models for copying an existing dataset. Now, we evaluate how they can be used to generate datasets from scratch. For each strategy, we created a dataset as described in \autoref{sec:gen}; these five datasets were then aggregated to evaluate the fit of the linear models described \autoref{fig:meancov_strat}.
The resulting fits are shown \autoref{fig:comp_gen_strat}. Model 1 (top panel) fits well on the \ide dimension $(\mu_i, \sigma_i)$, but fails on the \mmt dimension $(\mu_t, \sigma_t)$, and the Pearson r. In contrast, Model 2 and 3 (middle and bottom panels) are a lot closer to \autoref{fig:meancov_strat}, with Model 3 on the whole superior in about every metric.

\begin{figure}[htbp]
	\centering
	\makebox[\textwidth]{
		\includegraphics[width=1.3\textwidth]{img/tcop_strat.pdf}}
	\makebox[\textwidth]{
		\includegraphics[width=1.3\textwidth]{img/emg_strat.pdf }}
	\makebox[\textwidth]{
		\includegraphics[width=1.3\textwidth]{img/emg_control_strat.pdf }}

	\caption{An evaluation of the linear parametrization of the bivariate Gaussian model of \autoref{eq:gaussian_ide_mt}. Each subpanel reads like those of \autoref{fig:meancov_strat}. Top panel: evaluation for Model 1. Middle panel: evaluation for Model 2. Bottom panel: evaluation for Model 3.}
	\label{fig:comp_gen_strat}
	\Description{An evaluation of the linear parametrization of the bivariate Gaussian model of \autoref{eq:gaussian_ide_mt}. Each subpanel reads like those of \autoref{fig:meancov_strat}. Top panel: evaluation for Model 1. Middle panel: evaluation for Model 2. Bottom panel: evaluation for Model 3.}
\end{figure}




\section{Conclusion and Future Work}
We presented a comprehensive analysis of user pointing behaviors under different speed-accuracy strategies, leveraging existing datasets. Based on this analysis, we proposed and compared three generative models that are parameterized by these strategies.


In summary, our contributions are:
\begin{itemize}
	\item Introduction of Copulas to HCI: We introduced the statistical tool of copulas to the field of HCI and demonstrated how it can be used to describe dependencies in pointing data. Given their flexibility, copulas have the potential to model dependencies across various types of interaction data.
	\item Advancing Knowledge of Pointing Models: We enhanced the understanding of pointing models by describing the joint distributions of the Index of Difficulty (\ide) and Movement Time (MT) for various strategies. We demonstrated that mapping strategies onto the interval [-1,1] allows for straightforward linear parameterization.
	\item Proposing and Evaluating Generative Models: We proposed and evaluated three models for generating realistic pointing data using criteria relevant to HCI researchers. Our findings suggest that no single model should be considered universally superior. Model 3 is optimal for replicating empirical data when average values (such as \ide and \mmt) are significant. Model 2 is preferable for applications focusing on the ``min'' Fitts' law. Model 1 is most suitable when low \ide values are of interest.
	\item Providing Resources for Future Research: We have made the parameters used to generate realistic data available in the paper and will shared the code used in this work (access will be provided after the first round of reviews).
\end{itemize}


For future research, we recommend exploring hierarchical pointing models as proposed in~\cite{zhao2022}. Notably, our supplementary materials reveal considerable inter-participant differences in \ide distributions across strategies: some participants display highly consistent distributions, indicating strong internal strategy adherence, while others perform faster conditions more precisely than accurate ones. Similarly, there are notable differences among participants in the variability of MTs.


\bibliographystyle{ACM-Reference-Format}
\bibliography{hierarchical_model}

\appendix

\section{Fitting copulas \label{app:copula_method}}
We considered copulas from the most widely recognized families: elliptical (\eg the $t$-copula), Archimedean (\eg Clayton, Gumbel), and extreme value (\eg HR, Galambos, $t$-EV), as well as their rotated variants\footnote{Copulas can exhibit dependencies in the lower tails (low values) or upper tails (high values) of distributions. Given that pointing data in our dataset displays high variance at higher ID levels, copulas with upper tail dependence, such as the Gumbel copula, tend to perform poorly. By rotating the copula, we switch the dependence from the upper tail to the lower tail, providing additional copula candidates for consideration.}.
We utilized the R \texttt{copula} package~\cite{yan2007} to fit these candidate copulas; copulas are estimated based on maximum likelihood estimation, and the marginals are estimated with \textit{empirical} maximum likelihood estimation.
For generating repeated measures at prescribed \ide levels, we wrote custom code that leverages the conditional copula.



\section{Proofs \label{app:proofs}}
In this proof, we compute correlation coefficients between MT and \ide, and \mmt and \ide. Interestingly, we show any distribution with a linear conditional expectation leads to $r^2(\mmt, \ide) = 1$ whatever its conditional variance. Note that in this proof, we use the mathematical expectation (\ie population averages) rather than sample averages --- in practice this means these results require a large sample size to be exactly true.



\paragraph{Result: }  \hyperlink{res:method_2__correlation}{\textbf{Pearson's $r$ for method 2}}
We define
\begin{itemize}
	\item $Y=\ide$ and $X=\text{MT}$
	\item $\mathbb{E}$ the mathematical expectation
	\item $\mu_X = \mathbb{E}[X]$ the mean of the random variable $X$
	\item $\sigma_X \equiv \sqrt{\text{Var}(X)} = \sqrt{\mathbb{E}[(X - \mathbb{E}[X])^2]}$ the standard deviation of the random variable $X$
\end{itemize}
The Pearson correlation coefficient is defined as a normalized covariance
\begin{align}
	r(X,Y) = \frac{\mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y]}{\sigma_X \sigma_Y}.
\end{align}

We write $\mathbb{E}[X|Y] = f(Y), \text{Var}[X|Y] = g(Y)$. We have that

\begin{align}
	\mathbb{E}[XY] & = \mathbb{E}[Y\mathbb{E}[X|Y]]   =      \mathbb{E}[Yf(Y)]   \label{eq:proof_1}                   \\
	\mathbb{E}[X]  & = \mathbb{E}[\mathbb{E}[X|Y]] = \mathbb{E}[f(Y)]           \label{eq:proof_2}                    \\
	\text{Var}(X)  & = \mathbb{E}[\text{Var}(X|Y)] + \text{Var}(\mathbb{E}[X|Y])                   \label{eq:proof_3} \\
	               & = \mathbb{E}[g(Y)] + \text{Var}(f(Y))
\end{align}

The general practice of computing block averages is equal to considering $\overline{X} = \mathbb{E}[X|Y]$ instead of $X$:

\begin{align}
	r(\overline{X},Y) = \frac{\mathbb{E}[\overline{X}Y] - \mathbb{E}[\overline{X}]\mathbb{E}[Y]}{\sigma_{\overline{X}} \sigma_Y}.
\end{align}
One notices from \autoref{eq:proof_1} and \autoref{eq:proof_2} that interestingly, the covariance between $X$ and $Y$ equals the covariance between $\overline{X}$ and $Y$. We then have
\begin{align}
	\sigma_{\overline{X}}^2 = \text{Var}(\mathbb{E}[X|Y]) = \text{Var}(f(Y)).
\end{align}
When compared with \autoref{eq:proof_3}, one thus sees that the only difference between $r(X,Y)$ and $r(\overline{X},Y)$ is that $\mathbb{E}[\text{Var}(X|Y)]$ is not present in the denominator. This shows $r(\overline{X},Y)$ is always larger than $r(X,Y)$, and explains the practice of block averages.

\paragraph{Linear conditional expectation and standard deviation leads to perfect correlation.}
It is known that pointing data reaches very high values of $r(\overline{X},Y)$, often above $.9$, and sometimes even above $.99$. Here, we show that a linear model of conditional expectation and standard deviation will reach $r(\overline{X},Y) = 1$.

Linear conditional expectation and standard deviation read
\begin{align}
	\mathbb{E}[X|Y] & = a + b Y  = f(Y)     \\
	\text{Var}[X|Y] & = (c + d Y)^2 = g(Y).
\end{align}
This implies
\begin{align}
	r(X,Y)  = \frac{\mathbb{E}[Y(a + bY)] - \mathbb{E}[a + bY]\mathbb{E}[Y] }{\sqrt{(\mathbb{E}[(c + dY)^2] + \text{Var}(a + bY))}\sigma_Y} \\
	r(\overline{X}, Y) = \frac{\mathbb{E}[Y(a + bY)] - \mathbb{E}[a + bY]\mathbb{E}[Y] }{\sqrt{\text{Var}(a + bY)}\sigma_Y}.
\end{align}

The covariance part simplifies as
\begin{align}
	\mathbb{E}[Y(a + bY)] - \mathbb{E}[a + bY]\mathbb{E}[Y] & = a \mu_Y + b\mathbb{E}[Y^2] - a\mu_Y + b\mathbb{E}[Y]^2 \\
	                                                        & = b\sigma_Y^2.
\end{align}

Then
\begin{align}
	\text{Var}(a + bY) = b^2\text{Var}(Y) = b^2\sigma_Y^2,
\end{align}
which leads to $r(\overline{X}, Y) = 1$. Note that $g(Y)$ does not play a role in this, as the variance of $X$ for a given $Y$ is nullified when considering $\overline{X}$.

The extra term in $r(X,Y)$ can also be evaluated as
\begin{align}
	\mathbb{E}[(c + dY)^2] = \mathbb{E}[c^2 + 2cdY + d^2Y^2] \text{ } \\
\end{align}
and some further algebra shows
\begin{align}
	\sqrt{(\mathbb{E}[(c + dY)^2] + \text{Var}(a + bY))} = \sqrt{c^2 + \mu_Y(2cd + d^2) + (d^2 + b^2)\text{Var}(Y)},
\end{align}
which offers no further simplification for $r(X,Y)$.


This implies that the dependence between \ide and MT can not be specified independently of the EMG parameters, even though all possible values of $r(X,Y)$ are theoretically attainable:
\begin{itemize}
	\item If $b^2\,\sigma^2_Y >> c^2 +\mu_Y(2cd + d^2)$, then $\sigma_X \to \sqrt{b^2 + d^2} \sigma_Y$ and $r(X,Y) \to \frac{b}{\sqrt{b^2 + d^2}} $. If, in addition, $b >> d$, this reduces to $r(X,Y) = \text{sign}(b)$ \ie $\pm 1$. In practice, this can occur if $b$ is very large, which is unusual.
	\item If $c^2 +\mu_Y(2cd + d^2) >> b^2\,\sigma^2_Y$, then $\sigma_Y << \sigma_X$ and $r(X,Y) \to 0$.
\end{itemize}


\section{The WHo model \label{app:who}}
The Who model by Guiard and Rioul~\cite{guiard2015} is an axiomatic model that conforms to the following 5 axioms:
\begin{enumerate}
	\item[a1] There is a minimum movement time MT$_0$ achievable
	\item[a2] If we define relative precision as $y = \sigma/D$ with $\sigma$ the standard deviation of endpoints, then there is a minimum relative precision $y_0$ achievable.
	\item[a3] The function $f$ that links MT to $y$, $y=f(\text{MT})$ is decreasing and convex
	\item[a4] For a given participant effort $k$, there is an operation such that $MT \odot y = k$
	\item[a5] Effort invested is never total
\end{enumerate}
The WHo model
\begin{align}
	(y-y_0)^{1-\alpha} (\text{MT}-\text{MT}_0)^{\alpha} = k
\end{align}
satisfies a(1) -- a(4), and is fit on the convex hull of the scatterplot in the (MT, $y$) space to satisfy a(5).
The WHo model is \textit{not} a description of distribution of movement times, instead it characterizes the ``best'' samples (axioms 2 and 5), which is operationalized by fitting not on all data points but only on the convex hull of the dataset.
Guiard and Rioul never described what the distribution of movement times could look like within their WHo model.





\end{document}
